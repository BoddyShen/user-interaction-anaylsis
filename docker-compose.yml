version: '3.10'

networks:
  spark-network:
    driver: bridge

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    platform: linux/amd64
    networks:
      - spark-network

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9093, EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT 
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CLUSTER_NAME: "MyKafkaCluster"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./create-topics.sh:/usr/bin/create-topics.sh
      - kafka_data:/var/lib/kafka/data
      - kafka_logs:/var/log/kafka
    user: root 
    entrypoint:
      - sh
      - -c
      - |
        #!/bin/bash
        /etc/confluent/docker/run &
        sleep 10
        /usr/bin/create-topics.sh
        sleep infinity
    networks:
      - spark-network

  spark-master:
    container_name: da-spark-master
    image: da-spark-image
    entrypoint: ['./entrypoint.sh', 'master']
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 5s
      timeout: 3s
      retries: 3
    volumes:
      - ./spark_apps:/opt/spark/apps
      - spark-logs:/opt/spark/spark-events
    env_file:
      - .env.spark
    ports:
      - '9090:8080'
      - '7077:7077'
    networks:
      - spark-network

  spark-worker:
    image: da-spark-image
    entrypoint: ['./entrypoint.sh', 'worker']
    depends_on:
      - spark-master
    env_file:
      - .env.spark
    volumes:
      - ./spark_apps:/opt/spark/apps
      - spark-logs:/opt/spark/spark-events
    networks:
      - spark-network

  spark-history-server:
    container_name: da-spark-history
    image: da-spark-image
    entrypoint: ['./entrypoint.sh', 'history']
    depends_on:
      - spark-master
    env_file:
      - .env.spark
    volumes:
      - spark-logs:/opt/spark/spark-events
    ports:
      - '18080:18080'
    networks:
      - spark-network

volumes:
  kafka_data:
  kafka_logs:
  spark-logs: